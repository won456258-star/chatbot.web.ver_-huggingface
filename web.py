import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceHub
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í—ˆê¹…í˜ì´ìŠ¤ API í† í° ì„¤ì •
# Streamlit Secretsë‚˜ ì§ì ‘ ì½”ë“œë¥¼ í†µí•´ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ì´ ì˜ˆì œì—ì„œëŠ” .env íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡", layout="wide")
st.title("ğŸ“„ PDF ë¬¸ì„œì™€ ëŒ€í™”í•˜ëŠ” ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("PDF íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("ì—¬ê¸°ì— PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")
    st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš© ì²˜ë¦¬ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")

# ë²¡í„° ì €ì¥ì†Œë¥¼ ì„¸ì…˜ ìƒíƒœì— ì´ˆê¸°í™”
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None

# ëŒ€í™” ê¸°ë¡ì„ ì„¸ì…˜ ìƒíƒœì— ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# í•¨ìˆ˜: PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 
def process_pdf(file):
    if file is not None:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_file_path = f"./temp_{file.name}"
        with open(temp_file_path, "wb") as f:
            f.write(file.getvalue())

        # PDF ë¡œë”
        loader = PyPDFLoader(temp_file_path)
        documents = loader.load()

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_file_path)
        return chunks
    return None

# í•¨ìˆ˜: ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def create_vector_store(chunks):
    if chunks:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vector_store = FAISS.from_documents(chunks, embedding=embeddings)
        return vector_store
    return None

# PDF íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬ ì‹œì‘
if uploaded_file is not None and st.session_state.vector_store is None:
    with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        # 1. PDF ì²˜ë¦¬
        text_chunks = process_pdf(uploaded_file)
        
        # 2. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if text_chunks:
            st.session_state.vector_store = create_vector_store(text_chunks)
            st.success("PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ì§ˆë¬¸ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.error("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_question = st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

if user_question and st.session_state.vector_store:
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    llm = HuggingFaceHub(
        repo_id="mistralai/Mixtral-8x7B-Instruct-v0.1",
        model_kwargs={"temperature": 0.1, "max_length": 1024}
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ìµœì‹  ë°©ì‹)
    template = """
    ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸(input)ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    [Context]:
    {context}
    
    [Question]:
    {input}
    
    [Answer]:
    """
    prompt = ChatPromptTemplate.from_template(template)

    # RAG ì²´ì¸ ìƒì„± (ìµœì‹  ë°©ì‹)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = st.session_state.vector_store.as_retriever()
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # ì²´ì¸ í˜¸ì¶œ
                result = retrieval_chain.invoke({"input": user_question})
                response = result.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.write(response)
                # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

elif user_question:
    st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")