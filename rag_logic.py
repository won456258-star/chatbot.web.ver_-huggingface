# íŒŒì¼ëª…: rag_logic.py

import streamlit as st
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

# web.pyì—ì„œ ì—ëŸ¬ë¥¼ ì¡ì•„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì‚¬ìš©ì ì •ì˜ ì—ëŸ¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
class RagChainInitializationError(Exception):
    pass

@st.cache_resource
def get_rag_chain(api_key: str):
    """
    Hugging Face API í‚¤ë¥¼ ì¸ìë¡œ ë°›ì•„ RAG ì²´ì¸ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜¤ë¥˜ ë°œìƒ ì‹œ RagChainInitializationErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
    """
    try:
        # 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        embeddings = SentenceTransformerEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # 2. FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
        vectorstore = FAISS.load_local(
            "my_faiss_db",
            embeddings,
            allow_dangerous_deserialization=True
        )
        retriever = vectorstore.as_retriever()

        # 3. LLM ì´ˆê¸°í™” (ì „ë‹¬ë°›ì€ API í‚¤ ì‚¬ìš©)
        HUGGING_FACE_MODEL_ID = "google/gemma-2b-it"

        llm = HuggingFaceEndpoint(
            repo_id=HUGGING_FACE_MODEL_ID,
            huggingfacehub_api_token=api_key,
            task="text-generation",
            max_new_tokens=512,
            temperature=0.1,
        )

        # 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        template = """ë‹¹ì‹ ì€ 'ëª¨êµ¬' ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ ì£¼ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸:
        {context}

        ì§ˆë¬¸:
        {question}

        ë‹µë³€:
        """
        prompt = ChatPromptTemplate.from_template(template)

        # 5. RAG ì²´ì¸ êµ¬ì„±
        rag_chain = (
            {
                "context": itemgetter("question") | retriever,
                "question": itemgetter("question"),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        return rag_chain

    except Exception as e:
        # ğŸš« st.error()ë¥¼ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ , ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ web.pyì— ì•Œë¦½ë‹ˆë‹¤.
        raise RagChainInitializationError(f"RAG ì²´ì¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")