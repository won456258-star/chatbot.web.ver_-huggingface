# íŒŒì¼ëª…: rag_logic.py (HuggingFaceEndpoint ì‚¬ìš© ìµœì¢… ë²„ì „)

import streamlit as st
import os
from langchain_community.embeddings import SentenceTransformerEmbeddings
# âœ… ë³€ê²½ì : HuggingFaceHub ëŒ€ì‹  HuggingFaceEndpointë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

# --- Hugging Face í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---
def check_hf_api_token():
    if os.getenv("HUGGINGFACEHUB_API_TOKEN"):
        return True
    else:
        return False

@st.cache_resource
def get_rag_chain():
    # 1. API í† í° ë¡œë“œ í™•ì¸
    if not check_hf_api_token():
        return None

    try:
        # 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        embeddings = SentenceTransformerEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # 3. FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
        vectorstore = FAISS.load_local(
            "my_faiss_db",
            embeddings,
            allow_dangerous_deserialization=True
        )
        retriever = vectorstore.as_retriever()

        # 4. LLM ì´ˆê¸°í™” (âœ… HuggingFaceEndpoint í´ë˜ìŠ¤ë¡œ ë³€ê²½)
        HUGGING_FACE_MODEL_ID = "google/gemma-2b-it"

        llm = HuggingFaceEndpoint(
            repo_id=HUGGING_FACE_MODEL_ID,
            task="text-generation", # ğŸ‘ˆ ìƒì„± ëª¨ë¸ì„ì„ ëª…ì‹œ
            max_new_tokens=512,
            temperature=0.1,
        )

        # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        template = """ë‹¹ì‹ ì€ 'ëª¨êµ¬' ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ ì£¼ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸:
        {context}

        ì§ˆë¬¸:
        {question}

        ë‹µë³€:
        """
        prompt = ChatPromptTemplate.from_template(template)

        # 6. RAG ì²´ì¸ êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼)
        rag_chain = (
            {
                "context": itemgetter("question") | retriever,
                "question": itemgetter("question"),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        return rag_chain

    except Exception as e:
        st.error(f"RAG ì²´ì¸ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None